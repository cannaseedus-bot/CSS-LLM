model:
  layers: 8
  hidden_size: 768
  num_heads: 12
  intermediate_size: 3072
  vocab_size: 32000
  max_position_embeddings: 1024
  norm_type: rmsnorm
  activation: swiglu

training:
  batch_size: 512
  lr: 3e-4
  weight_decay: 0.01
  warmup_steps: 2000
  total_steps: 500000
  adam_beta1: 0.9
  adam_beta2: 0.95

data:
  dataset: openwebtext
  tokenizer: bpe-32k
