model:
  layers: 8
  hidden_size: 768
  num_heads: 12
  intermediate_size: 3072
  vocab_size: 32000
  max_position_embeddings: 1024

training:
  warmup_steps: 1000
  total_steps: 50000
  batch_size: 128
  lr: 3e-4
  tokens_target: 200000000
  adam_beta1: 0.9
  adam_beta2: 0.95
  weight_decay: 0.01

data:
  sources:
    - tinyshakespeare.txt
    - wikitext-2
  tokenizer: bpe-32k
